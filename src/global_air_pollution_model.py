# -*- coding: utf-8 -*-
"""global air pollution model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19hbn7O1faQ54cNR7KU8X7XB3q1d8NXRp
"""

import pandas as pd
# Load the dataset
df = pd.read_csv('global air pollution dataset.csv')
# Display the first few rows to understand it
print(df.head())

#AQI means Air Quality Index
df.info()

df.describe()

df.isnull().sum()

#There are missing values on the Country column and City
#We cannot fill the categorical values with mean or meadian
#If we fill those rows with mode or Unknown it's goining to affect the model performance.
#In conclution we can remve those null rows because in country column there are 428 rows null
#therer are total of 23463 rows availbale so 428/23463 = 1.8% so if we romove the null rows it's not goining to affect the model performance.

# Print the number of rows before dropping
print("Rows before dropping:", len(df))

# Drop rows where 'Country' or 'City' is null
# inplace=True means the change will happen directly to our 'df' dataframe

df.dropna(subset=['Country', 'City'], inplace=True)

# Print the number of rows after dropping to see the change
print("Rows after dropping:", len(df))

# Check for missing values again
print("\nMissing values after cleaning:")
print(df.isnull().sum())

#Remove Data Leakages
# List of columns to drop
columns_to_drop = [
    'AQI Value',
    'AQI Category',
    'CO AQI Category',
    'Ozone AQI Category',
    'NO2 AQI Category',
    'PM2.5 AQI Category'
]

# Drop the columns from the DataFrame
# We are creating a new DataFrame 'df_cleaned' to be safe
df_cleaned = df.drop(columns=columns_to_drop, axis=1)

# Display the first few rows of the new cleaned DataFrame to check
print("DataFrame after dropping leaky columns:")
print(df_cleaned.head())

# Drop the 'City' column because of the Curse of Dimensionality

print("Shape before dropping City:", df_cleaned.shape)
df_cleaned = df_cleaned.drop(columns=['City'])
print("Shape after dropping City:", df_cleaned.shape)

# Check the new columns
print("\nNew columns:", df_cleaned.columns)

print("Original shape:", df_cleaned.shape)

#Encording Country using One-Hot Encoding
#If we get the City Column we have to Encode it as well
# Apply One-Hot Encoding using pandas get_dummies
# This will find all text columns and convert them

print("\nStarting One-Hot Encoding...")
df_encoded = pd.get_dummies(df_cleaned, columns=['Country'], drop_first=True)

# Display the first few rows of the new encoded DataFrame
print("\nDataFrame after One-Hot Encoding:")
print(df_encoded.head())

# Check the new shape. You will see a lot more columns!
print("\nNew shape:", df_encoded.shape)

# Define the Target (y)
# We want to predict the 'PM2.5 AQI Value'

y = df_encoded['PM2.5 AQI Value']

# Define the Features (X)
# X is everything ELSE, so we drop the target column from the DataFrame

X = df_encoded.drop(columns=['PM2.5 AQI Value'], axis=1)

# Check the shapes to make sure it's correct
print("Shape of X (Features):", X.shape)
print("Shape of y (Target):", y.shape)

# You can also look at the first few rows of each
print("\nFirst 5 rows of X:")
print(X.head())
print("\nFirst 5 values of y:")
print(y.head())

# Train The model

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check the shapes of the new sets
print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

from sklearn.linear_model import LinearRegression

# Create an empty instance of the Linear Regression model
# We'll call our model 'lr_model' (Linear Regression)

lr_model = LinearRegression()

# Train the model using our training data
# This is where the model "learns"
print("Training the Linear Regression model...")
lr_model.fit(X_train, y_train)
print("Training complete!")

# Use the trained model (lr_model) to make predictions on the test data (X_test)
print("Making predictions on the test data...")
y_pred_lr = lr_model.predict(X_test)
print("Predictions are ready!")

# You can look at the first 5 predictions and compare them with the first 5 actual values
print("\nFirst 5 Predictions:", y_pred_lr[:5])
print("First 5 Actual Values:", y_test.values[:5])

import numpy as np
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

r2_lr = r2_score(y_test, y_pred_lr)
mae_lr = mean_absolute_error(y_test, y_pred_lr)

# First, calculate the Mean Squared Error (MSE)
mse_lr = mean_squared_error(y_test, y_pred_lr)
# Then, get the square root of the MSE to find the RMSE
rmse_lr = np.sqrt(mse_lr)

print("\n--- Linear Regression Model Evaluation ---")
print(f"R-squared (R²) Score: {r2_lr:.4f}")
print(f"Mean Absolute Error (MAE): {mae_lr:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse_lr:.4f}")

from sklearn.ensemble import RandomForestRegressor
import numpy as np

# 1. Create an empty instance of the Random Forest Regressor model
# n_estimators=100 means it will build 100 decision trees
# random_state=42 ensures we get the same result every time
# n_jobs=-1 tells the model to use all available CPU power to speed up training

rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)

#Train the model using the SAME training data
print("Training the Random Forest Model...")

rf_model.fit(X_train,y_train)
print("Training Complete")

#Make predictions with the new, powerful model

print("\nMaking predictions with the Random Forest model...")
y_pred_rf = rf_model.predict(X_test)

#Evaluate the new model using RMSE calculation

r2_rf = r2_score(y_test, y_pred_rf)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
mse_rf = mean_squared_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mse_rf)

#Print the new results AND compare them with the Linear Regression results

print("\n--- Model Comparison ---")
print("                    Random Forest   |   Linear Regression")
print("----------------------------------------------------------")
print(f"R-squared (R²) Score:   {r2_rf:.4f}          |   {r2_lr:.4f}")
print(f"Mean Absolute Error (MAE):  {mae_rf:.4f}         |   {mae_lr:.4f}")
print(f"Root Mean Squared Error (RMSE):{rmse_rf:.4f}         |   {rmse_lr:.4f}")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Get the feature importances from the trained Random Forest model
importances = rf_model.feature_importances_

# Get the names of the features
feature_names = X_train.columns

# Create a DataFrame to view them together
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})

# Sort the DataFrame to see the most important features at the top
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Visualize the Top 10 most important features
plt.figure(figsize=(10, 6)) # Set the size of the plot
sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(10)) # Plot a bar chart
plt.title('Top 10 Most Important Features for Predicting PM2.5 AQI')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.show()

# Print the top 10 features
print("\nTop 10 most important features:")
print(feature_importance_df.head(10))